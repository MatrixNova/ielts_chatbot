{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query and Message Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from mistralai import Mistral\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successful\n"
     ]
    }
   ],
   "source": [
    "if not load_dotenv(\".env\"):\n",
    "    print(\"An error has occured. Make sure the file exists and is readable\")\n",
    "else:\n",
    "    print(\"Loading successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Pinecone():\n",
    "    try:\n",
    "        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    "        if not pinecone_api_key:\n",
    "            print(\"An error has occured. Pinecone API key not found\")\n",
    "            return None, None\n",
    "\n",
    "        pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "        index_name= os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "        if not index_name:\n",
    "            print(\"An error has occured. PINECONE_INDEX_NAME not found in environmental variables\")\n",
    "            return None, None\n",
    "\n",
    "        print(f\"Verifying Pinecone Index. Create one if needed.\")\n",
    "        if index_name not in [idx.name for idx in pc.list_indexes()]:\n",
    "            print(f\"Pinecone Index {index_name} not found. Creating one\")\n",
    "            pc.create_index_for_model(\n",
    "                name=index_name,\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\",\n",
    "                embed={\n",
    "                    \"model\": \"text-embedding-multilingual-e5-large\",\n",
    "                    \"field_map\": {\"text\": \"text\"}\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index {index_name} created in Pinecone\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Index {index_name} already exists in Pinecone\")\n",
    "\n",
    "        index = pc.Index(index_name)\n",
    "        print(f\"Connecting to Pinecone Index object 'index_name'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occured during Pinecone initialization process: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return pc, index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message generating function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM clients - Mistral and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm_clients():\n",
    "    if Mistral:\n",
    "        try:\n",
    "            mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "            if not mistral_api_key:\n",
    "                print(\"An error has occured. Mistral API key not found.\")\n",
    "            \n",
    "            else:\n",
    "                mistral_client = Mistral(api_key=mistral_api_key)\n",
    "                print(\"Mistral client successfully initialized.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error has occured during Mistral initialization process: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skipping Mistral client initialization - client library not found\")\n",
    "\n",
    "    if OpenAI:\n",
    "        try:\n",
    "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not openai_api_key:\n",
    "                print(\"An error has occured. OpenAI API key not found.\")\n",
    "            \n",
    "            else:\n",
    "                openai_client = OpenAI(api_key=openai_api_key)\n",
    "                print(\"OpenAI client succesfully initialized.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error has occured during OpenAi initialization process: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping OpenAI client initialization - client library not found\")\n",
    "\n",
    "    return mistral_client, openai_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query relevant passages from Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_passage(query, top_k = 3):\n",
    "    if not pc or not index:\n",
    "        print(\"An error has occured. Pinecone client or index cannot be initialize globally\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        embedding_responses = pc.inference.embed(\n",
    "            model=\"multilingual-e5-large\",\n",
    "            inputs=[query],\n",
    "            parameters={\"input_type\": \"query\"}\n",
    "            )\n",
    "\n",
    "        if not embedding_responses or not getattr(embedding_responses, 'data', None) or not embedding_responses.data[0]['values']:\n",
    "            print(\"An error has occured. Failure in generating query embedding or unexpected response structure\")\n",
    "            return None\n",
    "        \n",
    "        query_embedding = embedding_responses.data[0]['values']\n",
    "\n",
    "        query_responses = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            namespace=os.getenv(\"PINECONE_NAMESPACE\")\n",
    "        )    \n",
    "        \n",
    "        passages = [match[\"metadata\"][\"text\"] for match in query_responses[\"matches\"]]\n",
    "        return passages\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"An error has occured when trying to access Pinecone embedding response.\")\n",
    "        print(\"Embedding response object: \", embedding_responses)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error has occured during Pinecone query or embedding process: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reading passages using selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reading_passages(model_choice, query, passages):\n",
    "    context = \" \".join(passages).strip()\n",
    "\n",
    "    # Fallback if Pinecone cannot find the suitable passages\n",
    "    if not context:\n",
    "        context = f\"A passage about: {query}\"\n",
    "        \n",
    "    prompt = f\"\"\"You are an IELTS Reading expert. Based on the following context, generate an IELTS-style academic reading passage.\n",
    "\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "Please follow this structure:\n",
    "\n",
    "Passage Guidelines:\n",
    "- The passage should be approximately 700–800 words long.\n",
    "- Use an academic tone, similar to passages found in the IELTS Reading section.\n",
    "- Organize the content into 4–6 paragraphs.\n",
    "- Include a title that reflects the main idea of the passage.\n",
    "- Do NOT include any questions or answers.\n",
    "- Do NOT add extra instructions, labels, or headings such as “Questions” or “Answers.”\n",
    "\n",
    "Do not include explanations or justifications unless explicitly asked by the user. Your output must be directly usable in an IELTS reading practice application.\n",
    "\"\"\"\n",
    "    try:\n",
    "        if model_choice == \"Mistral\":\n",
    "            if not mistral_client:\n",
    "                raise ValueError(\"Mistral client is not available\")\n",
    "            \n",
    "            response = mistral_client.chat.complete(\n",
    "                model = \"mistral-small-latest\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            passage_content = response.choices[0].message.content\n",
    "        \n",
    "        elif model_choice == \"GPT 4.1\":\n",
    "            if not openai_client:\n",
    "                raise ValueError(\"OpenAI client is not available\")\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            passage_content = response.choices[0].message.content\n",
    "        \n",
    "        else:\n",
    "            print(f\"An error has occured. Invalid model choice: {model_choice}. Please choose 'Mistral' or 'GPT 4.1'.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occured during API call for {model_choice}. Please try again.\")\n",
    "        return None\n",
    "        \n",
    "    return passage_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate IELTs Reading structured questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(model_choice, passage):\n",
    "    prompt = f\"\"\"\n",
    "You are an IELTS Reading expert that generates IELTs-style reading questions based on a providede passage.\n",
    "Your task is to output ONLY a valid JSON array containing exactly 10 object questions based on the passage below:\n",
    "\n",
    "\\\"\\\"\\\"{passage}\\\"\\\"\\\"\n",
    "\n",
    "Please follow this structure:\n",
    "\n",
    "Question Types:\n",
    "Alternate between the following IELTS question types:\n",
    "1. Multiple choice (4 options: A, B, C, D)\n",
    "2. Identifying information (True / False / Not Given)\n",
    "3. Identifying writer’s views/claims (Yes / No / Not Given)\n",
    "4. Matching headings / information / features / sentence endings\n",
    "5. Sentence completion\n",
    "6. Short-answer questions\n",
    "\n",
    "Instructions:\n",
    "- Generate a well-balanced mix of these types.\n",
    "- Ensure questions reflect the style and difficulty of the IELTS Reading section.\n",
    "- Each question should be clear and concise.\n",
    "- For multiple choice, label the options clearly as A, B, C, D.\n",
    "- Do NOT provide answers at the end.\n",
    "\n",
    "Output Formatting Requirement (MANDATORY):\n",
    "- Your **ENTIRE** responses MUST be a single, valid JSON array.\n",
    "- The response MUST start with '[' and end with ']'.\n",
    "- Each element in the array MUST be a JSON object with the following EXACT keys:\n",
    "    1. \"number\": (integer) The question number (01-10).\n",
    "    2. \"type\": (string) The specific question types (e.g. \"Multiple Choices\", \"True/False/Not Given, etc.\")\n",
    "    3. \"text\": (string) The full question texts (including options A, B, C, D for multiple choice questions).\n",
    "- **CRITICAL**: DO NOT include ANY text, explanation, introduction, section headers (such as \"Section 1: Multiple Choices\"),\n",
    "or ANY other content BEFORE the opening '['or AFTER the closing ']'.\n",
    "- The output MUST be machine-readable JSON only.\n",
    "\n",
    "Example of a single object within the required JSON array format:\n",
    "     \"number\": 1,\n",
    "     \"type\": \"Multiple choice\",\n",
    "     \"text\": \"What is the primary topic discussed?\\\\nA) Option A text\\\\nB) Option B text\\\\nC) Option C text\\\\nD) Option D text\"\n",
    "\n",
    "Generate the JSON array based on the provided passage.\n",
    "     \n",
    "\"\"\"\n",
    "    try:\n",
    "        if model_choice == \"Mistral\":\n",
    "            if not mistral_client:\n",
    "                raise ValueError(\"Mistral client is not available\")\n",
    "            \n",
    "            response = mistral_client.chat.complete(\n",
    "                model=\"mistral-small-latest\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            raw_questions = response.choices[0].message.content\n",
    "\n",
    "        elif model_choice == \"GPT 4.1\":\n",
    "            if not openai_client:\n",
    "                raise ValueError(\"OpenAI client is not available\")\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            raw_questions = response.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            print(f\"An error has occured. Invalid model choice: {model_choice}. Please choose 'Mistral' or 'GPT 4.1'.\")\n",
    "            return None\n",
    "        \n",
    "        if raw_questions:\n",
    "            cleaned_questions = re.sub(r\"^```json\\s*|\\s*```$\", \"\", raw_questions.strip())\n",
    "\n",
    "            try:\n",
    "                questions_list = json.loads(cleaned_questions)\n",
    "            except json.JSONDecodeError as json_err:\n",
    "                 print(f\"Error decoding JSON response from AI: {json_err}\")\n",
    "                 print(\"Raw response was:\\n\", raw_questions)\n",
    "                 questions_list = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occured during API call for {model_choice}. Please try again.\")\n",
    "        return None\n",
    "    \n",
    "    return questions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing Query and Script Generation process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral client successfully initialized.\n",
      "OpenAI client succesfully initialized.\n",
      "Verifying Pinecone Index. Create one if needed.\n",
      "Index ielts-rag already exists in Pinecone\n",
      "Connecting to Pinecone Index object 'index_name'.\n",
      "\n",
      " Process the query Explain the process of photosynthesis using Mistral model\n",
      "\n",
      "============================== RESULTS ==============================\n",
      "\n",
      "--- Generated Passage ---\n",
      "### The Ecological Significance of Light and Seed Production in Plants\n",
      "\n",
      "The role of light in the natural world is multifaceted and crucial for the survival and growth of various organisms. One of the most well-documented phenomena is the adaptive significance of seed production in plants, particularly in species like bamboo. Bamboo exhibits a unique reproductive strategy where it produces an enormous quantity of seeds simultaneously. This mass seeding event can result in seeds being layered 12 to 15 centimeters deep on the ground. The adaptive significance of this strategy is clear: the sheer volume of seeds overwhelms the capacity of seed-eating animals to consume them all. Consequently, a significant number of seeds escape predation and are able to germinate, ensuring the continuation of the species (Evans, 1976). This strategy is a testament to the plant's ability to adapt to its environment and maximize its reproductive success.\n",
      "\n",
      "Light plays a pivotal role in the life of plants, not only as a trigger for seed production but also as an essential component for photosynthesis. Photosynthesis is the process by which plants convert light energy, typically from the sun, into chemical energy stored in organic compounds. This process is fundamental to the growth and development of plants, as it allows them to synthesize the necessary organic materials from carbon dioxide and water. The rate of photosynthesis in a plant can be quantified by measuring the rate of carbon uptake. This metric provides valuable insights into the plant's photosynthetic efficiency and its response to varying light conditions.\n",
      "\n",
      "The photosynthetic responses of plants to light intensity are diverse and can be categorized into different patterns. Some plants achieve maximal photosynthesis at relatively low light intensities, such as one-quarter of full sunlight. These plants are often adapted to shaded environments where light is limited. In contrast, other plants, like sugarcane, do not reach a maximum photosynthetic rate but continue to increase their photosynthetic activity as light intensity rises. This adaptability allows them to thrive in environments with high light availability. The variability in photosynthetic responses highlights the evolutionary adaptations of plants to different ecological niches, ensuring their survival and growth in a wide range of conditions.\n",
      "\n",
      "In addition to its role in photosynthesis, light also influences other critical aspects of plant development. For instance, light quality and duration can affect the timing of flowering, the orientation of leaves, and the overall growth patterns of plants. Photoperiodism, the response of plants to the length of day or night, is a well-studied phenomenon that regulates various developmental processes. Plants use light as a cue to synchronize their growth with seasonal changes, ensuring that they reproduce and grow at optimal times. This synchronization is crucial for their survival and reproductive success, as it allows them to take advantage of favorable environmental conditions.\n",
      "\n",
      "The interaction between light and plant growth is further complicated by the presence of other environmental factors. Temperature, water availability, and soil nutrients all play significant roles in modulating the plant's response to light. For example, plants in arid regions may have adapted to maximize water use efficiency, which can affect their photosynthetic rates and overall growth. Similarly, nutrient-deficient soils can limit the plant's ability to utilize light energy effectively, leading to reduced growth and productivity. Understanding these interactions is essential for developing sustainable agricultural practices and conserving plant biodiversity.\n",
      "\n",
      "In conclusion, the ecological significance of light and seed production in plants is profound and multifaceted. From the adaptive strategies of mass seeding to the intricate processes of photosynthesis, light plays a central role in the life of plants. The diverse responses of plants to light intensity and quality underscore their remarkable ability to adapt to a wide range of environmental conditions. By studying these processes, scientists can gain valuable insights into plant ecology and develop strategies to enhance agricultural productivity and conserve natural ecosystems.\n",
      "\n",
      "--- Generated Questions (JSON) ---\n",
      "[\n",
      "    {\n",
      "        \"number\": 1,\n",
      "        \"type\": \"Multiple choice\",\n",
      "        \"text\": \"What is the primary adaptive strategy of bamboo mentioned in the passage?\\nA) Producing seeds in small quantities over a long period\\nB) Producing an enormous quantity of seeds simultaneously\\nC) Relying on animals to disperse its seeds\\nD) Growing in areas where there are no seed-eating animals\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 2,\n",
      "        \"type\": \"True/False/Not Given\",\n",
      "        \"text\": \"The passage states that the rate of photosynthesis can be measured by the rate of oxygen release.\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 3,\n",
      "        \"type\": \"Yes/No/Not Given\",\n",
      "        \"text\": \"Does the writer agree that the interaction between light and plant growth is straightforward?\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 4,\n",
      "        \"type\": \"Matching sentence endings\",\n",
      "        \"text\": \"Choose the correct ending for the following sentence:\\nThe adaptive significance of bamboo's mass seeding event is that\\nA) it ensures that all seeds are consumed by animals\\nB) it allows the plant to conserve energy for other processes\\nC) it overwhelms the capacity of seed-eating animals to consume them all\\nD) it promotes the growth of other plant species\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 5,\n",
      "        \"type\": \"Sentence completion\",\n",
      "        \"text\": \"Plants that achieve maximal photosynthesis at relatively low light intensities are often adapted to __________ environments.\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 6,\n",
      "        \"type\": \"Short-answer question\",\n",
      "        \"text\": \"What is the term used to describe the response of plants to the length of day or night?\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 7,\n",
      "        \"type\": \"Multiple choice\",\n",
      "        \"text\": \"According to the passage, what can limit a plant's ability to utilize light energy effectively?\\nA) High temperatures\\nB) Nutrient-deficient soils\\nC) Excessive water availability\\nD) High light intensity\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 8,\n",
      "        \"type\": \"True/False/Not Given\",\n",
      "        \"text\": \"The passage suggests that understanding the interactions between light and other environmental factors is crucial for sustainable agriculture.\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 9,\n",
      "        \"type\": \"Yes/No/Not Given\",\n",
      "        \"text\": \"Does the writer agree that the diverse responses of plants to light intensity and quality are remarkable?\"\n",
      "    },\n",
      "    {\n",
      "        \"number\": 10,\n",
      "        \"type\": \"Sentence completion\",\n",
      "        \"text\": \"The process by which plants convert light energy into chemical energy is called __________.\"\n",
      "    }\n",
      "]\n",
      "Query and Script Generation process completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nExecuting Query and Script Generation process\")\n",
    "\n",
    "    # Declare globally\n",
    "    mistral_client, openai_client = initialize_llm_clients()\n",
    "    pc, index = initialize_Pinecone()\n",
    "\n",
    "    if pc and index:\n",
    "        try:\n",
    "            # Generate an example\n",
    "            query = \"Explain the process of photosynthesis\"\n",
    "            chosen_LLM = \"Mistral\"\n",
    "\n",
    "            print(f\"\\n Process the query {query} using {chosen_LLM} model\")\n",
    "            \n",
    "            retrieved_passages = query_passage(query)\n",
    "            if retrieved_passages is not None:\n",
    "                generate_passages = generate_reading_passages(chosen_LLM, query, retrieved_passages)\n",
    "\n",
    "                if generate_passages:\n",
    "                    generate_questions = generate_questions(chosen_LLM, generate_passages)\n",
    "\n",
    "                    print(\"\\n\" + \"=\"*30 + \" RESULTS \" + \"=\"*30)\n",
    "                    print(\"\\n--- Generated Passage ---\")\n",
    "                    print(generate_passages)\n",
    "                    print(\"\\n--- Generated Questions (JSON) ---\")\n",
    "\n",
    "                    if generate_questions:\n",
    "                        print(json.dumps(generate_questions, indent=4))\n",
    "\n",
    "                    else:\n",
    "                        print(\"Failed to generate or parse questions.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Failed to generate passage.\")\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to retrieve passages from Pinecone.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error has occurred during the main execution flow: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"An error has occured during Pinecone client or index initialization process\")\n",
    "\n",
    "print(\"Query and Script Generation process completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
