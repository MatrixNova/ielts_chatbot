{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccc74f9",
   "metadata": {},
   "source": [
    "# Answers and Feedback Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3130b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bb8b0",
   "metadata": {},
   "source": [
    "## Load environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2963085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successful\n"
     ]
    }
   ],
   "source": [
    "if not load_dotenv(\".env\"):\n",
    "    print(\"An error has occured. Make sure the file exists and is readable\")\n",
    "else:\n",
    "    print(\"Loading successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f01928",
   "metadata": {},
   "source": [
    "## Initialize LLM clients - OpenAI and DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d20cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "<>:2: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20500\\468270316.py:2: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "  if 'OpenAI' in globals() and 'OpenAI' is not None:\n"
     ]
    }
   ],
   "source": [
    "def initialize_llm_clients():\n",
    "    if 'OpenAI' in globals() and 'OpenAI' is not None:\n",
    "        try:\n",
    "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not openai_api_key:\n",
    "                print(\"An error has occured. OpenAI API key not found.\")\n",
    "            \n",
    "            else:\n",
    "                openai_client = OpenAI(api_key=openai_api_key)\n",
    "                print(\"OpenAI client succesfully initialized.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error has occured during OpenAi initialization process: {e}\")\n",
    "    \n",
    "        try:\n",
    "            deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "            if not deepseek_api_key:\n",
    "                print(\"An error has occured. DeepSeek API key not found.\")\n",
    "\n",
    "            else:\n",
    "                deepseek_client = OpenAI(\n",
    "                    base_url=\"https://openrouter.ai/api/v1\",\n",
    "                    api_key=deepseek_api_key\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error has occured during DeepSeek initialization process: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping LLM initialization process. OpenAI library imported incorrectly\")\n",
    "\n",
    "    return openai_client, deepseek_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc8927",
   "metadata": {},
   "source": [
    "## Evaluate user's answers and provide feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726eabae",
   "metadata": {},
   "source": [
    "##### Grade the user's answers and explain the incorrect ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8619ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(model_choice, passage_content, questions_string, user_answers):\n",
    "    if not all([passage_content, questions_string, user_answers]):\n",
    "         print(\"Error: Missing passage, questions, or user answers for evaluation.\")\n",
    "         return None\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an IELTs Reading Expert. Your task is to evaluate the user's answers based on the provided passage and questions. \n",
    "    Provide the correct answers and grade the user's submission.\n",
    "\n",
    "    Passage:\n",
    "    \\\"\\\"\\\"{passage_content}\\\"\\\"\\\"\n",
    "\n",
    "    Questions:\n",
    "    \\\"\\\"\\\"{questions_string}\\\"\\\"\\\"\n",
    "\n",
    "    User's answers:\n",
    "    \\\"\\\"\\\"{user_answers}\\\"\\\"\\\"\n",
    "\n",
    "    Please follow this structure:\n",
    "    \n",
    "    Instructions:\n",
    "    1. Go through each question number found in the 'Questions' section.\n",
    "    2. For each question number, find the corresponding answer in the 'User's answers' section.\n",
    "    3. Evaluate if the user's answer is correct or incorrect based ONLY on the provided Passage.\n",
    "    4. State the correct answer based ONLY on the provided passage.\n",
    "    5. If the user's answer is incorrect, provide a brief explanation referencing the SPECIFIC part of the passage that supports the correct answer. \n",
    "    Keep explanations concise.\n",
    "    6. After evaluating all questions, count the number of 'Correct' answers.\n",
    "    7. Calculate the total number of questions evaluated based on the 'Questions' input section.\n",
    "    8. Calculate the score percentage (Number Correct / Total Questions * 100), rounded to the nearest whole number.\n",
    "    9. Format the output exactly as specified below, including the Detailed Evaluation and the Final Grade with the score percentage.\n",
    "\n",
    "    Format the output clearly as shown below:\n",
    "    ===DETAILED EVALUATION===\n",
    "    Question 1:\n",
    "    - Your answer: [user's answer for Question 1.]\n",
    "    - Evaluation: [Correct/Incorrect.]\n",
    "    - Correct answer: [Correct answer for Question 1.]\n",
    "    - Explanation: [Brief explanation referencing the passage if the user's answer is incorrect.\n",
    "    Write N/A if the answer is correct.]\n",
    "\n",
    "    Question 2:\n",
    "    - Your answer: [user's answer for Question 2.]\n",
    "    - Evaluation: [Correct/Incorrect.]\n",
    "    - Correct answer: [Correct answer for Question 2.]\n",
    "    - Explanation: [Brief explanation referencing the passage if the user's answer is incorrect.\n",
    "    Write N/A if the answer is correct.]\n",
    "\n",
    "    (Repeat this process for ALL questions numbered in the 'Question' input)\n",
    "\n",
    "    ===FINAL GRADE===\n",
    "    Total questions answered correctly: [Number of correct questions] / [Total number of questions]\n",
    "    Score Percentage: [Calculated Percentage]%\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_choice == \"GPT 4.1\":\n",
    "            if not openai_client:\n",
    "                raise ValueError(\"OpenAI client is not available\")\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4.1-2025-04-14\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor expert in evaluation and following output formats precisely.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            evaluation = response.choices[0].message.content \n",
    "\n",
    "        elif model_choice == \"DeepSeekR1\":\n",
    "            if not deepseek_client:\n",
    "                raise ValueError(\"DeepSeek client is not available\")\n",
    "            \n",
    "            response = deepseek_client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-r1:free\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an IELTs Reading tutor expert in evaluation and following output formats precisely.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            evaluation = response.choices[0].message.content \n",
    "        \n",
    "        else:\n",
    "            print(f\"An error has occured. Invalid model choice: {model_choice}. Please choose 'GPT 4.1' or 'DeepSeekR1'.\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error has occured during API call for {model_choice}: {e}. Please try again.\")\n",
    "        return None\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30d414",
   "metadata": {},
   "source": [
    "##### Provide feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "586a92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(evaluation_results, questions_data):\n",
    "    # Error handling for input\n",
    "    if not isinstance(evaluation_results, list) or not isinstance(questions_data, list):\n",
    "        print(\"An error has occured. Invalid input format for get_feedback. Expecting list\")\n",
    "        return \"Error generating feedback due to invalid input format\", []\n",
    "    \n",
    "    if not evaluation_results:\n",
    "        if questions_data:\n",
    "            return \"Could not parse the evaluation results. Unable to provide feedback\", []\n",
    "        else:\n",
    "            return \"No question or results detected to be evaluate\", []\n",
    "\n",
    "    # Map question numbers to types\n",
    "    questions_type_map = {\n",
    "        q.get('number'): q.get('type')\n",
    "        for q in questions_data\n",
    "        if isinstance(q, dict) and 'number' in q and 'type' in q\n",
    "    }\n",
    "\n",
    "    unknown_placeholder = \"Unknown types of questions\"\n",
    "    unknown_types = not bool(questions_type_map) and bool(questions_data)\n",
    "    if unknown_types:\n",
    "        print(\"Question formatting error. Could not map question types.\")\n",
    "    \n",
    "    # Count the incorrect answers by type\n",
    "    incorrect_answers_count_by_type = Counter()\n",
    "    struggling_type = set()\n",
    "    found_incorrect = False\n",
    "\n",
    "    for result in evaluation_results:\n",
    "        if isinstance(result, dict) and result.get('evaluation') == 'Incorrect':\n",
    "            found_incorrect = True\n",
    "            q_num = result.get('number')\n",
    "            q_type = questions_type_map.get(q_num) if q_num is not None else None\n",
    "\n",
    "            if q_type:\n",
    "                incorrect_answers_count_by_type[q_type] += 1\n",
    "                struggling_type.add(q_type)\n",
    "\n",
    "            else:\n",
    "                \"\"\" Only count unknown if if q_num exists but not in the map\n",
    "                or if q_num was missing in results\n",
    "                \"\"\"\n",
    "                incorrect_answers_count_by_type[unknown_placeholder] += 1\n",
    "                struggling_type.add(unknown_placeholder)\n",
    "\n",
    "        elif not isinstance(result, dict) or 'evaluation' not in result:\n",
    "            print(f\"An error has occurred. Skipping invalid item in evaluation results: {result}\")\n",
    "\n",
    "    # Refining the list of struggling questions types\n",
    "    struggling_type_list = sorted(list(struggling_type))\n",
    "\n",
    "    if unknown_placeholder in struggling_type and len(struggling_type) > 1:\n",
    "        struggling_type.remove(unknown_placeholder)\n",
    "        struggling_type_list = sorted(list(struggling_type))\n",
    "\n",
    "    # Feedback message generation:\n",
    "    feedback = \"===FEEDBACK===\\n\"\n",
    "    struggling_questions_identified = struggling_type_list and struggling_type_list != [unknown_placeholder]\n",
    "\n",
    "    if struggling_questions_identified:\n",
    "        feedback += \"Based on your performance, the types of questions you might want to practice more are: \\n\"\n",
    "        for q_type in struggling_type_list:\n",
    "            count = incorrect_answers_count_by_type[q_type]\n",
    "            feedback += f\"- {q_type} ({count} incorrect)\\n\"\n",
    "\n",
    "        if unknown_types and incorrect_answers_count_by_type[unknown_placeholder] > 0:\n",
    "             feedback += f\"- (Also {incorrect_answers_count_by_type[unknown_placeholder]} incorrect answers where the type couldn't be identified.)\\n\"\n",
    "        feedback += \"\\n\"\n",
    "\n",
    "    # Incorrect answers exist, but their types could not be identified    \n",
    "    elif found_incorrect:\n",
    "        count_unknown = incorrect_answers_count_by_type[unknown_placeholder]\n",
    "        \n",
    "        if count_unknown > 0:\n",
    "            feedback += f\"There are {count_unknown} incorrect answers. However, the types of questions cannot be specified for feedback this time\\n\"\n",
    "\n",
    "        else:\n",
    "            feedback += \"There are some incorrect answers, but their specific types cannot be determined.\"\n",
    "    \n",
    "    # No incorrect answer\n",
    "    elif evaluation_results:\n",
    "        feedback += \"Excellent! It looks like you have answered all questions correctly\\n\"\n",
    "\n",
    "    else:\n",
    "        feedback += \"No specific areas for improvements identified from this evaluation\\n\"\n",
    "    \n",
    "    # Generate follow up question\n",
    "    follow_up_questions = \"Congratulation on completing the Reading passage. What would you like to do next?\\n\"\n",
    "    option_letter = 'A'\n",
    "\n",
    "    # Option A: Practice with the types of questions the user is struggling with\n",
    "    if struggling_questions_identified:\n",
    "        if len(struggling_type_list) == 1:\n",
    "            types_str = struggling_type_list[0]\n",
    "        \n",
    "        elif len(struggling_type_list) == 2:\n",
    "            types_str = f\"{struggling_type_list[0]} and {struggling_type_list[1]}\"\n",
    "        \n",
    "        else:\n",
    "            types_str = \", \".join(struggling_type_list[:-1]) + f\" and {struggling_type_list[-1]}\"\n",
    "        \n",
    "        follow_up_questions += f\"{option_letter} Practice weak areas ({types_str})with new sets of questions\\n\"\n",
    "        option_letter = chr(ord(option_letter) + 1)        # Increase increment from A -> B\n",
    "\n",
    "    # Option B: Pratice with a new passage and new problem sets (or if A has no weak areas)\n",
    "    follow_up_questions += f\"{option_letter} Continue practicing with a new passage and sets of questions?\\n\"\n",
    "    option_letter = chr(ord(option_letter) + 1)            # Increase increment from B -> C\n",
    "\n",
    "    # Option C: Practice the same problem again with different question sets\n",
    "    follow_up_questions += f\"{option_letter} Retry the passage with different sets of questions?\\n\"\n",
    "    option_letter = chr(ord(option_letter) + 1)            # Increase increment from C -> D\n",
    "\n",
    "    # Option D: End the practice session\n",
    "    follow_up_questions += f\"Ending the practice session. Hope you have had a productive learning session\\n\"\n",
    "\n",
    "    valid_options = [chr(ord('A') + i) for i in range(ord(option_letter) - ord('A'))]\n",
    "    follow_up_questions += f\"Please enter your choice ({', '.join(valid_options)}): \\n\"\n",
    "\n",
    "    full_response = feedback + follow_up_questions\n",
    "    # Returns the list of types needed for option A in the orchestrator\n",
    "    types_for_practice = struggling_type_list if struggling_questions_identified else []\n",
    "\n",
    "    return full_response, types_for_practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ec28d",
   "metadata": {},
   "source": [
    "## Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a0707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Answer Evaluation and Feedback process\n",
      "OpenAI client succesfully initialized.\n",
      "\n",
      "Evaluating answers using GPT 4.1...\n",
      "\n",
      "============================== EVALUATION RESULTS ==============================\n",
      "===DETAILED EVALUATION===\n",
      "Question 1:\n",
      "- Your answer: Sunlight, water, CO2\n",
      "- Evaluation: Correct.\n",
      "- Correct answer: Sunlight, water, and carbon dioxide\n",
      "- Explanation: N/A\n",
      "\n",
      "Question 2:\n",
      "- Your answer: Leaves\n",
      "- Evaluation: Incorrect.\n",
      "- Correct answer: Chloroplasts\n",
      "- Explanation: The passage states: \"This process occurs in chloroplasts using chlorophyll.\" It does not mention 'leaves' explicitly.\n",
      "\n",
      "Question 3:\n",
      "- Your answer: False\n",
      "- Evaluation: Correct.\n",
      "- Correct answer: False\n",
      "- Explanation: Plants release oxygen, not nitrogen, during photosynthesis, as stated in the passage.\n",
      "\n",
      "===FINAL GRADE===\n",
      "Total questions answered correctly: 2 / 3\n",
      "Score Percentage: 67%\n",
      "\n",
      "Feedback Generation (Requires Parsing)\n",
      "(Skipping feedback generation as parsing the evaluation string is needed first)\n",
      "\n",
      " Answer Evaluation and Feedback Process completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nInitializing Answer Evaluation and Feedback process\")\n",
    "\n",
    "    openai_client, deepseek_client = initialize_llm_clients()\n",
    "\n",
    "    # Initiate the process using the example from query.ipynb:\n",
    "    example_passage_content = \"This is an example passage about photosynthesis. Plants use sunlight, water, and carbon dioxide to create their food (glucose) and release oxygen. This process occurs in chloroplasts using chlorophyll.\"\n",
    "\n",
    "    example_questions_string = \"\"\"\n",
    "    Question 1: What do plants use to make food?\n",
    "    Question 2: Where does photosynthesis occur?\n",
    "    Question 3: True/False/Not Given: Plants release nitrogen during photosynthesis.\n",
    "    \"\"\"\n",
    "\n",
    "    example_user_answers = \"\"\"\n",
    "    Answer 1: Sunlight, water, CO2\n",
    "    Answer 2: Leaves\n",
    "    Answer 3: False\n",
    "    \"\"\"\n",
    "\n",
    "    example_questions_data = [\n",
    "        {\"number\": 1, \"type\": \"Short-answer questions\", \"text\": \"What do plants use...\"},\n",
    "        {\"number\": 2, \"type\": \"Short-answer questions\", \"text\": \"Where does photosynthesis occur?\"},\n",
    "        {\"number\": 3, \"type\": \"True/False/Not Given\", \"text\": \"True/False/Not Given: Plants release nitrogen...\"}\n",
    "    ]\n",
    "\n",
    "    chosen_model = \"GPT 4.1\"\n",
    "\n",
    "    print(f\"\\nEvaluating answers using {chosen_model}...\")\n",
    "\n",
    "    # Call the evaluation function\n",
    "    evaluation_output_string = evaluate_answers(\n",
    "        chosen_model,\n",
    "        example_passage_content,\n",
    "        example_questions_string,\n",
    "        example_user_answers,x\n",
    "    )\n",
    "\n",
    "    if evaluation_output_string:\n",
    "        print(\"\\n\" + \"=\"*30 + \" EVALUATION RESULTS \" + \"=\"*30)\n",
    "        print(evaluation_output_string)\n",
    "\n",
    "        # --- Generate Feedback ---\n",
    "        print(\"\\nFeedback Generation (Requires Parsing)\")\n",
    "        print(\"(Skipping feedback generation as parsing the evaluation string is needed first)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nFailed to get evaluation results\")\n",
    "\n",
    "    print(\"\\n Answer Evaluation and Feedback Process completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
