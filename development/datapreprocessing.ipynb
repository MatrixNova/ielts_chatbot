{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successful\n"
     ]
    }
   ],
   "source": [
    "if not load_dotenv(\".env\"):\n",
    "    print(\"An error has occurred. Make sure the file exists and is readable\")\n",
    "else:\n",
    "    print(\"Loading successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish connection & Table Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname= os.getenv(\"POSTGRES_DBNAME\"),\n",
    "            user= os.getenv(\"POSTGRES_USER\"),\n",
    "            password= os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "            host= os.getenv(\"POSTGRES_HOST\"),\n",
    "            port= os.getenv(\"POSTGRES_PORT\")\n",
    "        )\n",
    "        print(\"Successfully connected to PostgreSQL\")\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Checking if the table 'passages' exist\n",
    "        print(\"Checking for 'passages' table\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT FROM information_schema.tables\n",
    "                WHERE table_name = 'passages'\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        table_exists = cur.fetchone()[0]\n",
    "        if not table_exists:\n",
    "            print(\"The table 'passages' does not exist. Creating one.\")\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE passages (\n",
    "                    passage_id SERIAL PRIMARY KEY,\n",
    "                    title TEXT,\n",
    "                    text TEXT\n",
    "                )\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "            print(\"'passages' table created.\")\n",
    "\n",
    "        else:\n",
    "            print(\"'passages' table already exists.\")\n",
    "\n",
    "        # Create 'processed_files' tracking table if it doesn't exist\n",
    "        print(\"Checking for 'processed_files' tracking table\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT FROM information_schema.tables\n",
    "                WHERE table_name = 'processed_files'\n",
    "            );\n",
    "        \"\"\")\n",
    "        processed_table_exists = cur.fetchone()[0]\n",
    "\n",
    "        if not processed_table_exists:\n",
    "            print(\"The table 'processed_files' does not exist. Creating one.\")\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE processed_files (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    filename TEXT UNIQUE NOT NULL,\n",
    "                    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "            print(\"'processed_files' table created.\")\n",
    "            \n",
    "        else:\n",
    "            print(\"'processed_files' table already exists.\")\n",
    "\n",
    "        print(\"Database setup completed\")\n",
    "        return conn, cur\n",
    "\n",
    "    # Catches errors related to the database \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error has occurred during connecting to PostgreSQL or when creating tables: {e}\")\n",
    "        if cur:\n",
    "            cur.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "        return None, None\n",
    "    \n",
    "    # Catches any other potential errors\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error has occurred: {e}\")\n",
    "        if cur:\n",
    "            cur.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "        \n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieves processed filenames from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_files(cur):\n",
    "    processed = set()\n",
    "\n",
    "    if not cur:\n",
    "        print(\"Database cursor not available.\")\n",
    "        return processed    \n",
    "    \n",
    "    try:\n",
    "        cur.execute(\"SELECT filename FROM processed_files\")\n",
    "        rows = cur.fetchall()\n",
    "        processed = set(row[0] for row in rows)\n",
    "\n",
    "        return processed\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error has occurred when fetching processed files: {e}\")\n",
    "        \n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a filename to the processed_files tracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_file_as_processed(conn, cur, filename):\n",
    "    if not cur or not conn:\n",
    "        print(f\"Database connection or cursor is not available for {filename}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "                    INSERT INTO processed_files (filename)\n",
    "                    VALUES (%s) ON CONFLICT (filename) \n",
    "                    DO NOTHING\"\"\", (filename,)\n",
    "                    ) \n",
    "        conn.commit()\n",
    "        return True\n",
    "    \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error marking file {filename} as processed: {e}\")\n",
    "        conn.rollback()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap text from a single PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_PDF(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    try:\n",
    "        with pymupdf.open(file_path) as pdf_document:\n",
    "            full_text = \" \".join(page.get_text() for page in pdf_document).strip()\n",
    "\n",
    "        lines = [line.strip() for line in full_text.splitlines() if line.strip()]\n",
    "\n",
    "        if not lines:\n",
    "            print(f\"An error has occurred. No text content extract from '{filename}'.\")\n",
    "\n",
    "            return filename.replace(\".pdf\", \"\"), \"\"\n",
    "\n",
    "        title = lines[0]\n",
    "        text = \"\".join(lines[1:]) if len(lines) > 1 else \"\"\n",
    "    \n",
    "        if not text.strip():\n",
    "            print(f\"Warning. No content extracted from the file {filename}\")\n",
    "\n",
    "        return title, text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error has occurred from {file_path}: {e}\")\n",
    "\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the texts before loading them into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_into_db(conn, cur, title, text):\n",
    "    if not cur or not conn:\n",
    "         print(\"Database connection or cursor is not available at the moment\")\n",
    "         return None\n",
    "          \n",
    "    try:\n",
    "        # Insert new passages into the Database\n",
    "        cur.execute(\"\"\"\n",
    "                    INSERT INTO passages (title, text)\n",
    "                    VALUES (%s, %s)\n",
    "                    RETURNING passage_id\"\"\", (title, text) \n",
    "                    )\n",
    "        passage_id = cur.fetchone()[0]\n",
    "\n",
    "        print(f\"Inserted text: '{title}' (Temporary ID: {passage_id})\")\n",
    "        \n",
    "        # Delete OLDER duplicates\n",
    "        cur.execute(\"\"\"\n",
    "                    DELETE FROM passages \n",
    "                    WHERE passage_id < %s\n",
    "                    AND title = %s\n",
    "                    AND text = %s;\n",
    "                    \"\"\", (passage_id, title, text))\n",
    "        \n",
    "        delete_count = cur.rowcount\n",
    "        if delete_count > 0:\n",
    "            print(f\"Remove {delete_count} older duplicates for '{title}'.\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Duplicates removed successfully!\")\n",
    "        \n",
    "        return passage_id\n",
    "\n",
    "    # Catches any database related errors\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error has occurred during loading text for '{title}': {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n",
    "\n",
    "    # Catches any other potential errors\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error has occurred: {e}\")\n",
    "        conn.rollback()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk text and update the database if there are new inserted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_passage_and_update_db(conn, cur, passage_id_to_be_replaced, title, full_text):\n",
    "    # Input validation\n",
    "    if not passage_id_to_be_replaced or not full_text:\n",
    "        print(f\"Skipping chunking for passage ID: {passage_id_to_be_replaced} ('{title}')\"\n",
    "              f\"due to missing ID or missing text.\")\n",
    "        return False\n",
    "    \n",
    "    if not cur or not conn:\n",
    "        print(\"Database connection or curser currently unavailable for chunking\")\n",
    "        return False\n",
    "    \n",
    "    # Chunking parameters to limit each chunk under Pinecone's threasehold\n",
    "    try:\n",
    "        chunk_size = 1000\n",
    "        overlap = 100\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(full_text):\n",
    "            end = start + chunk_size\n",
    "            chunks.append(full_text[start:end])     # Slice the text and add it to the list\n",
    "            next_start = start + chunk_size - overlap\n",
    "\n",
    "            # Avoid loop if overlap is bigger or equal to chunk size, or the text is too short\n",
    "            if next_start <= start:\n",
    "                break\n",
    "            start = next_start\n",
    "\n",
    "        if not chunks:\n",
    "            print(f\"An error has occurred. No chunks generated for passage ID {passage_id_to_be_replaced} ('{title}'). \"\n",
    "                  f\"Text might be too short or empty.\")\n",
    "\n",
    "            if full_text.strip():\n",
    "                return False                        # Return False when there was text but no chunks\n",
    "\n",
    "            else:\n",
    "                print(f\"Original text for passage ID {passage_id_to_be_replaced} ('{title}')\"\n",
    "                      f\"was empty, skipping chunking or duplicate deletion process.\")\n",
    "                \n",
    "                return True\n",
    "\n",
    "        # Delete original passage, then insert chunks\n",
    "        print(f\"Replacing passage with ID {passage_id_to_be_replaced} with {len(chunks)} chunks.\")\n",
    "        cur.execute(\"\"\"\n",
    "                    DELETE FROM passages WHERE passage_id = %s\n",
    "                    \"\"\", (passage_id_to_be_replaced,))\n",
    "        \n",
    "        if cur.rowcount == 0:\n",
    "            print(f\"Passage ID {passage_id_to_be_replaced} not found for deletion befor chunking. Proceed with chunk insertion.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Deleted original passage ID: {passage_id_to_be_replaced}\") \n",
    "\n",
    "        # Insert new chunks\n",
    "        inserted_chunks_id = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_title = f\"{title} (chunk {i + 1}/{len(chunks)})\"\n",
    "            cur.execute(\"\"\"\n",
    "                        INSERT INTO passages (title, text) \n",
    "                        VALUES (%s, %s)\n",
    "                        RETURNING passage_id\"\"\", (chunk_title, chunk_text))\n",
    "\n",
    "            new_id = cur.fetchone()[0]\n",
    "            inserted_chunks_id.append(new_id) \n",
    "            \n",
    "        conn.commit()\n",
    "        print(f\"Succesfully replaced passage ID {passage_id_to_be_replaced} with {len(chunks)}\"\n",
    "              f\"chunks (New ID: {inserted_chunks_id}).\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # Catches any potential database errors\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error has occurred during chunking or updating for original passage ID {passage_id_to_be_replaced}\"\n",
    "              f\"('{title}'): {e}\")\n",
    "        \n",
    "        conn.rollback()\n",
    "        return False\n",
    "    \n",
    "    # Catches any other potential errors\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error has occurred during chunking for original passage ID {passage_id_to_be_replaced}\"\n",
    "              f\"('{title}'): {e}\")\n",
    "        \n",
    "        conn.rollback()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan folders for new PDFs and process them incrementallly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_incremental(conn, cur):\n",
    "    if not cur or not conn:\n",
    "        print(\"Database connection or cursor currently unavailable. Incremental process cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Begin incremental process\n",
    "    processed_filenames = get_processed_files(cur)\n",
    "    print(f\"\\n Found {len(processed_filenames)} files previously processed\")\n",
    "\n",
    "    folder_path = os.getenv(\"PDF_FOLDER_PATH\")\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"An error has occurred. Source folder not found at '{folder_path}'. Please check the path again.\")\n",
    "        return\n",
    "\n",
    "    files_processed_count = 0\n",
    "    files_skipped_count = 0\n",
    "    files_error_count = 0\n",
    "\n",
    "    try:\n",
    "        pdf_files_in_folder = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
    "        print(f\"Found {len(pdf_files_in_folder)} PDF files in the target folder: {folder_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occurred during listing the files in folder '{folder_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Loop through each PDF inside the directory\n",
    "    for filename in pdf_files_in_folder:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if filename in processed_filenames:\n",
    "            files_skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # Processing for a new file\n",
    "        print(f\"\\n Processing new file: {filename}\")\n",
    "\n",
    "        title, text_content = extract_text_from_PDF(file_path)\n",
    "\n",
    "        # Check if the text extraction works or did the function fail completely\n",
    "        if title is None or text_content is None:\n",
    "            print(f\"Skipping file '{filename}' due to extraction error\")\n",
    "            files_error_count += 1\n",
    "            continue\n",
    "\n",
    "        # Check if the text is extracted but it is just empty space or not\n",
    "        if not text_content.split():\n",
    "            print(f\"Skipping file '{filename}' because extracted text content is empty\")\n",
    "            files_error_count += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"Extracted text from '{filename}'. Title '{title[:50]}'\")\n",
    "\n",
    "        # Save the full text to the database temporarilly\n",
    "        print(f\"Load the full text for '{filename}' into the database temporarilly\")\n",
    "        temp_passage_id = load_text_into_db(conn, cur, title, text_content)\n",
    "\n",
    "        # Verify if the full text is saved\n",
    "        if temp_passage_id:\n",
    "            print(f\"Full text loaded (ID: {temp_passage_id}). Attempting to chunk\")\n",
    "\n",
    "            chunking_success = chunk_passage_and_update_db(conn, cur, temp_passage_id, title, text_content)\n",
    "\n",
    "            if chunking_success:\n",
    "                print(f\"Chunking successful for '{filename}'. Proceed with the marking process\")\n",
    "\n",
    "                marking_success = mark_file_as_processed(conn, cur, filename)\n",
    "\n",
    "                if marking_success:\n",
    "                    files_processed_count += 1\n",
    "                    print(f\"Successfully processed and marked '{filename}'.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Chunking is succesful for '{filename}' but marking it as processed FAILED. Manual checking is required.\")\n",
    "                    files_error_count += 1\n",
    "            else:\n",
    "                print(f\"Chunking failed for '{filename}'. File not marked as processed.\")\n",
    "                files_error_count += 1\n",
    "        \n",
    "        else:\n",
    "            print(f\"Loading text into the Database failed for '{filename}'. Chunking process cannot be\"\n",
    "                \"processed. File not marked as processed\")\n",
    "\n",
    "    print(\"\\n Incremental processing completed\")\n",
    "    print(f\"\\n New files processed successfully: {files_processed_count}\")\n",
    "    print(f\"\\n Files skipped (already processed): {files_skipped_count}\")\n",
    "    print(f\"\\n Files with errors (skipped/failed): {files_error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to PostgreSQL\n",
      "Checking for 'passages' table\n",
      "'passages' table already exists.\n",
      "Checking for 'processed_files' tracking table\n",
      "'processed_files' table already exists.\n",
      "Database setup completed\n",
      "\n",
      " Found 19 files previously processed\n",
      "Found 19 PDF files in the target folder: D:\\IELTs Practice Web Platform\\Training samples (IELTs)\n",
      "\n",
      " Incremental processing completed\n",
      "\n",
      " New files processed successfully: 0\n",
      "\n",
      " Files skipped (already processed): 19\n",
      "\n",
      " Files with errors (skipped/failed): 0\n",
      "\n",
      "Closing database connection\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    conn_main, cur_main = establish_connection()\n",
    "\n",
    "    # Proceed only if connection was successful\n",
    "    if conn_main and cur_main:\n",
    "        try:\n",
    "            process_data_incremental(conn_main, cur_main)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during the main script execution: {e}\")\n",
    "        \n",
    "        finally: # Ensure closure happens even if errors occur\n",
    "            print(\"\\nClosing database connection\")\n",
    "            try:\n",
    "                if cur_main: cur_main.close()\n",
    "                if conn_main: conn_main.close()\n",
    "                print(\"Database connection closed.\")\n",
    "            except psycopg2.Error as e:\n",
    "                print(f\"Error closing database connection: {e}\")\n",
    "    else:\n",
    "        print(\"Script exiting: Database connection could not be established during setup.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
